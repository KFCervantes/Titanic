---
title: "Titanic"
author: "Kaleb Cervantes"
date: "3/29/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading Libraries.

```{r}
library(dplyr)
library(ggplot2)
library(knitr)
library(magrittr)
library(stringr)
library(titanic)
```

# Only Numeric

This part is to just learn some basic logistical reggression in R. In order to do this, I decided to subset the data to only include numeric columns and rows containing non null values.

```{r}
train <- titanic_train %>%
  select_if(is.numeric) %>%
  subset(!(Age %>% is.na))

train %>%
  head %>%
  kable
```

In order to check colinearity, I decided to use a correlation matrix.

```{r}
train %>%
  cor %>%
  kable
```

Since there does not seem to be colinearity, we procede.

```{r}
full_formula <- Survived ~ .

full_formula %>%
  glm(
    'binomial',
    train
  ) %>%
  summary
```

Here we see that the variables `PassengerId` and `Fare` do not seem significant, so for now I will drop them.

```{r}
lm1 <- (Survived ~ . - PassengerId - Fare) %>%
  glm(
    'binomial',
    train
  )

lm1 %>%
  summary
```

Now I only really understand the coefficents part of this. I will round the fitted values and compare them to the original to see what percentage I got correct.

```{r}
100 * ((lm1 %$% fitted.values %>% round) == (train %$% Survived)) %>%
  mean
```

This method seems to have correctly guessed 69.60784% of the dataset.

# With Factors

```{r}
titanic_train %>%
  head %>%
  kable
```

It is important to note that many of the values for `Cabin` are blank. These are technically not null values, but they probably should be treated as such. Assuming that `Name` does not contain duplicate values, it may help to drop that column as well.

Before creating the training dataframe, we may have to check the frequency of the values in `Cabin` and `Ticket`.

```{r}
titanic_train %>%
  subset(Cabin != '') %>%
  count(Cabin) %$%
  n %>%
  summary
```

Here we can see that -- for the most part -- the non blank values for `Cabin` only appear once. It may be helpful to drop this variable.

```{r}
titanic_train %>%
  count(Ticket) %$%
  n %>%
  summary
```

We see the same thing happen with `Ticket`, so we also drop this variable.

We now use this with the logistical model again.

We now create are training data.

```{r}
train2 <- titanic_train %>%
  subset(
    !(Age %>% is.na),
    -c(Cabin, Name, Ticket)
  )

train %>%
  head %>%
  kable
```

```{r}
full_formula %>%
  glm(
    'binomial',
    train2
  ) %>%
  summary
```

In this case, the only significant variables seems to be `Pclass`, `Sex`, `Age`, and `SibSp`. As such I decided to make a new model with only these.

```{r}
lm2 <- (Survived ~ Pclass + Sex + Age + SibSp) %>%
  glm(
    'binomial',
    train2
  )

lm2 %>%
  summary
```

This model seems to do well. In order to check the accuracy, I decided to use the same method as before.

```{r}
100 * ((lm2 %$% fitted.values %>% round) == (train2 %$% Survived)) %>%
  mean
```

This method seems to have correctly predicted 80.81232% of the dataset.

# Dealing with `Age`

I decided to look at what the training data is like for the significant variables.

```{r}
titanic_test[c('Pclass', 'Sex', 'Age', 'SibSp')] %>%
  summary
```

Here I noticed that `Age` has a lot of null values. In order to fix this, I decided to use a linear model to estimate age.

```{r}
lm_age <- (Age ~ . - Survived) %>%
  lm(train2) %>%
  step(
    trace = 0,
    k = train2 %>% nrow %>% log
  )

lm_age %>%
  summary

lm_age %>%
  plot
```

It seems like the variability seems to increase with the fitted values. The residuals also seem to be normally distributed. This indicates that the constant variability assumption is not met for linear regression.

So not all the assumptions for linear regression are met and the model does not do a great job of explaining variability. I'm probably still going to use this anyway.

In order to test this out, I added this estimate to the training data

```{r}
train3 <- titanic_train

ind <- train3 %$%
  Age %>%
  is.na

train_pred_age <- lm_age %>%
  predict(train3[ind, ])

# change negative age to min age
train_pred_age[train_pred_age < 0.17] <- 0.17

train3$Age[ind] <- train_pred_age

train3 %$%
  Age %>%
  summary
```

This seemed to get rid of the null values.

I now tested the accuracy of the last logistical model with this info.

```{r}
estimates <- lm2 %>%
  predict(
    train3,
    type = 'response'
  ) %>%
  round

100 * (estimates == train3$Survived) %>%
  mean
```

With these null values filled out, I was able to corretly guess 81.59371% of the results.

# Test Data

Now I will fill out the null values with the estimates.
```{r}
test <- titanic_test

ind <- test %$%
  Age %>%
  is.na

test_pred_age <- lm_age %>%
  predict(test[ind, ])

# change negative age to min age
test_pred_age[test_pred_age < 0.17] <- 0.17

test$Age[ind] <- test_pred_age

test %$%
  Age %>%
  summary
```

Now I estimate the predictions and output to a csv file.
```{r}
test_survived <- lm2 %>%
  predict(
    test,
    type = 'response'
  ) %>%
  round

data.frame(
  PassengerId = test$PassengerId,
  Survived = test_survived
) %>%
  write.csv('answer.csv')
```